# -*- coding: utf-8 -*-
"""msbd6000h_project.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1L1w5F1k7Brbq8nz5uUEAXYnlVMI3b72l

## Library Import
"""

from google.colab import drive
drive.mount('/content/drive')

# Commented out IPython magic to ensure Python compatibility.
# %%capture
# !pip install transformers

import os
import numpy as np
import pandas as pd
import ast
import nltk
import torch
import torch.nn as nn
import keras
import tensorflow as tf
import matplotlib.pyplot as plt
import string

from collections import Counter
from sklearn.model_selection import StratifiedKFold
from sklearn.metrics import accuracy_score, f1_score
from keras import backend as K
from keras.models import Sequential, Model
from keras.layers import Dense, Activation, Embedding, Dropout, BatchNormalization, Activation, Input, \
    Conv1D, MaxPool1D, Flatten, Concatenate, Add
from numpy.random import seed
from scipy.sparse import coo_matrix
from transformers import BertTokenizer, BertModel, AdamW, get_linear_schedule_with_warmup
from torch.utils.data import Dataset, DataLoader, RandomSampler, SequentialSampler

nltk.download('stopwords') 

stopwords = set(nltk.corpus.stopwords.words("english") + list(string.punctuation) + ['``', "''"])
ps = nltk.stem.PorterStemmer()

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

"""# Data Prep"""

def load_data(file_name):
    df = pd.read_csv(file_name)
    # stance_dict = {"SUPPORT":1, "NULL": 0, "OPPOSE": -1}
    label_dict = {"IMPACTFUL": 2, "MEDIUM_IMPACT": 1, "NOT_IMPACTFUL": 0, "UNKNOWN": -1}
    
    context = [ast.literal_eval(l) for l in df['context']]
    stances = [ast.literal_eval(l) for l in df['stance_label']]
    labels = [label_dict[k] for k in df["impact_label"]]
    joined_text = [context[i] + [df["text"][i]] for i in range(len(df["text"]))]
    joined_text = [" ".join(l) for l in joined_text]

    return list(df["id"]), list(df['text']), context, joined_text, stances, labels, df

train_id, train_texts, train_contexts, train_joined_texts, train_stance_labels, train_labels, train_df = load_data('/content/drive/MyDrive/HKUST/MSBD6000H/project/train.csv')
valid_id, valid_texts, valid_contexts, valid_joined_texts, valid_stance_labels, valid_labels, valid_df = load_data('/content/drive/MyDrive/HKUST/MSBD6000H/project/valid.csv')
test_id, test_texts, test_contexts, test_joined_texts, test_stance_labels, test_labels, test_df = load_data('/content/drive/MyDrive/HKUST/MSBD6000H/project/test.csv')

"""We combine the train and valid data to perform K-fold cross validation"""

# train_id = np.array(train_id + valid_id)
# train_texts = np.array(train_texts + valid_texts)
# train_contexts += valid_contexts
# train_joined_texts = np.array(train_joined_texts + valid_joined_texts)
# train_stance_labels += valid_stance_labels
# train_labels = np.array(train_labels + valid_labels)

"""# Feature Extraction"""

def tokenize(text):
    return nltk.word_tokenize(text)

def stem(tokens):
    return [ps.stem(token) for token in tokens]

def tokenize_and_stem(text):
    return stem(tokenize(text))

def filter_stopwords(tokens):
    return [token for token in tokens if token not in stopwords and not token.isnumeric()]

def remove_stop_words(texts):
    return ' '.join([word for word in texts.split() if word not in stopwords])

def n_gram(tokens, n=1):
    if n == 1:
        return tokens
    results = list()
    for i in range(len(tokens)-n+1):
        results.append(" ".join(tokens[i:i+n]))
    return results

def get_onehot_vector(feats, feats_dict):
    """
    :param feats: a list of features, type: list
    :param feats_dict: a dict from features to indices, type: dict
    return a feature vector,
    """
    # initialize the vector as all zeros
    vector = np.zeros(len(feats_dict), dtype=np.float)
    for f in feats:
        # get the feature index, return -1 if the feature is not existed
        f_idx = feats_dict.get(f, -1)
        if f_idx != -1:
            # set the corresponding element as 1
            vector[f_idx] = 1
    return vector

"""Since stances are relative to the parent, i.e. OPPOSE -> OPPOSE means that the text is overally supporting the topic. So, we convert the stance sequence into overall stances"""

def get_overall_stance(stances):
    stance_dict = {"SUPPORT": 1, "NULL": 1, "OPPOSE": -1}
    arr = [stance_dict[i] for i in stances[1:]]
    for i in range(1, len(arr)):
        arr[i] = arr[i-1] * arr[i]
    return arr

train_overall_stances = np.array([get_overall_stance(stances) for stances in train_stance_labels])
test_overall_stances = np.array([get_overall_stance(stances) for stances in test_stance_labels])

"""Since many data entries shares the same topics, that means that sentences with an impact label may have been shown previously, we can use it to help predict the descendent's impact. For example, opposing an impactful argument may be more likely to be ineffective compare to opposing a not impactful argument."""

text_dict = {train_texts[i]: train_labels[i] for i in range(len(train_labels))}
train_contexts_impact = [[text_dict.get(s, -1)+1 for s in sents[1:]] for sents in train_contexts]
test_contexts_impact = [[text_dict.get(s, -1)+1 for s in sents[1:]] for sents in test_contexts]

def get_parent_text(contexts):
    parents = [l[-1].lower() for l in contexts]
    return parents

train_text_parents = get_parent_text(train_contexts)
test_text_parents = get_parent_text(test_contexts)

"""# Exploratory Data Analysis

Here we explore more about each data entry to try to discover relationships between `text`, `context`, `stance_label`, and `impact_label`
"""

t = 44

z = zip(train_stance_labels[t], [0] + train_overall_stances[t], train_contexts[t] + [train_texts[t]])
for a, b, c in z:
    print('{message: <{fill}}'.format(message=a, fill='7'), b, text_dict.get(c, -1) + 1, c)

"""## Class distribution
The training data is dominated by IMPACTFUL data members, so model will be biased towards to and lead to imbalanced learning. 

If we need to use deep learning, then we need to oversample other classes or assign class weights.

On the other hand, Naive Bayes can handle the imbalance since the prior accounts for the label distribution.
"""

train_df['impact_label'].value_counts(normalize=0)

valid_df['impact_label'].value_counts(normalize=0)

"""## Sentence distribution
We now take a look at the total sentences in the training set, we notice that many sentences are indeed duplicates, meaning that multiple entries shares the same root and parent nodes. 

Moreover, we also observed that there are many sentences in the validation and test set that are also in the training set.

Therefore, we can utilize this nature and force the model to memorize what each sentences lead to.
"""

all_train_sents = list(train_texts) + [sent for sents in train_contexts for sent in sents]
train_sents_set = set(all_train_sents)
print("total number of sentences: {}, total number of unique sentences: {}".format(len(all_train_sents), len(train_sents_set)))

all_valid_sents = list(valid_texts) + [sent for sents in valid_contexts for sent in sents]
valid_sents_set = set(all_valid_sents)

all_test_sents = list(test_texts) + [sent for sents in test_contexts for sent in sents]
test_sents_set = set(all_test_sents)

v0, v1 = 0, 0
t0, t1 = 0, 0

for k in valid_sents_set:
    if k in train_sents_set:
        v0 += 1
    else:
        v1 += 1

for k in test_sents_set:
    if k in train_sents_set:
        t0 += 1
    else:
        t1 += 1

print("number of valid sentences in train: {}, number of test sentences in train: {}".format(v0,t0))
print("number of valid sentences not in train: {}, number of test sentences not in train: {}".format(v1,t1))

"""Next, we look at number of topics in the training set since we know from above that many nodes shares the same root.

Morever, we can look at the number of roots in valid and test sets that exists in the training set as well.
"""

all_train_topics = [sents[0] for sents in train_contexts]
train_topics_set = set(all_train_topics)
print("total number of topics: {}, total number of unique topics: {}".format(len(all_train_topics), len(train_topics_set)))

all_valid_topics = [sents[0] for sents in valid_contexts]
valid_topics_set = set(all_valid_topics)

all_test_topics = [sents[0] for sents in test_contexts]
test_topics_set = set(all_test_topics)

v0, v1 = 0, 0
t0, t1 = 0, 0

for k in valid_topics_set:
    if k in train_topics_set:
        v0 += 1
    else:
        v1 += 1

for k in test_topics_set:
    if k in train_topics_set:
        t0 += 1
    else:
        t1 += 1

print("number of valid topics in train: {}, number of test topics in train: {}".format(v0,t0))
print("number of valid topics not in train: {}, number of test topics not in train: {}".format(v1,t1))

"""## Stats per label"""

impact_indices = (train_labels == 2)
med_impact_indices = (train_labels == 1)
no_impact_indices = (train_labels == 0)

def histogram(l, bins=None):
    plt.hist(l, bins=bins)
    plt.show()
    print("average: {}, median: {}".format(np.mean(l), np.median(l)))

"""### Text length"""

train_text_tokens = np.array([filter_stopwords(stem(tokenize(t.lower()))) for t in train_texts])
train_text_len = np.array([len(t) for t in train_text_tokens])

impact_texts = train_text_tokens[impact_indices]
impact_texts_len = train_text_len[impact_indices]

med_impact_texts = train_text_tokens[med_impact_indices]
med_impact_texts_len = train_text_len[med_impact_indices]

no_impact_texts = train_text_tokens[no_impact_indices]
no_impact_texts_len = train_text_len[no_impact_indices]

histogram(impact_texts_len)
histogram(med_impact_texts_len)
histogram(no_impact_texts_len)

"""Note the median of the text input is 12 for IMPACTFUL, 14 for MEDIUM_IMPACTFUL, 11 for NOT_IMPACTFUL across both training and validation set. Perhaps this can be used as a feature"""

c = Counter(train_labels[train_text_len <= 11])
c[0] / len(no_impact_texts), c[1] / len(med_impact_texts), c[2] / len(impact_texts)

c = Counter(train_labels[train_text_len >= 12])
c[0] / len(no_impact_texts), c[1] / len(med_impact_texts), c[2] / len(impact_texts)

"""we can see that the probability distribution of text length do have some correlation with impact levels. With length <= 11, it is more likely to be not impactful. While with length >= 12, it is more likely to be medium impactful.

### Parent-to-text length 

from some training samples, we can see that text that are longer than parent context are more likely to be impactful. For example, longer text means the argument are more detailed which is a sign of good argument.
"""

train_parent_arguments = [l[-1] if len(l) > 1 else '' for l in train_contexts]
train_parent_tokens = np.array([filter_stopwords(stem(tokenize(t.lower()))) for t in train_parent_arguments])
train_parent_len = np.array([len(t) for t in train_parent_tokens])

len_diff = train_parent_len - train_text_len

impact_len_diff = len_diff[impact_indices]

med_impact_len_diff = len_diff[med_impact_indices]

no_impact_len_diff = len_diff[no_impact_indices]

histogram(impact_len_diff)
histogram(med_impact_len_diff)
histogram(no_impact_len_diff)

histogram(train_labels[len_diff == -2])

"""### Word distribution"""

impact_freq = nltk.FreqDist(w for s in impact_texts for w in s)

impact_freq.plot(40,cumulative=False)
print("total number of unique tokens: {}".format(len(impact_freq)))

"""unique tokens tied to impactful:
human, women, govern, societi, 
"""

med_impact_freq = nltk.FreqDist(w for s in med_impact_texts for w in s)

med_impact_freq.plot(40,cumulative=False)
print("total number of unique tokens: {}".format(len(med_impact_freq)))

"""unique tokens tied to med impact:
democrat, state, aid, countri, govern, wizard, develop, polici
"""

no_impact_freq = nltk.FreqDist(w for s in no_impact_texts for w in s)

no_impact_freq.plot(40,cumulative=False)
print("total number of unique tokens: {}".format(len(no_impact_freq)))

all_text_top_freq = impact_freq + med_impact_freq + no_impact_freq

all_text_top_freq = impact_freq.most_common(3000) + med_impact_freq.most_common(3000) + no_impact_freq.most_common(3000)

all_text_top_words = set([word for word, count in all_text_top_freq])

len(all_text_top_words)

"""### N-gram

### Stance

**Text stance vs impact**
"""

impact_stances = train_overall_stances[impact_indices]
impact_text_stances = [l[-1] for l in impact_stances]

med_impact_stances = train_overall_stances[med_impact_indices]
med_impact_text_stances = [l[-1] for l in med_impact_stances]

no_impact_stances = train_overall_stances[no_impact_indices]
no_impact_text_stances = [l[-1] for l in no_impact_stances]

histogram(impact_text_stances)
histogram(med_impact_text_stances)
histogram(no_impact_text_stances)

"""It is more likely to be impactful if text is overall supportive, equally likely to be medium impactful if text is either supportive or opposing, and more likely to be not impactful if text is overall opposing

**Number of parents vs impact**
"""

histogram(impact_stance_len, range(1,13))
histogram(med_impact_stance_len, range(1,13))
histogram(no_impact_stance_len, range(1,13))

histogram(impact_curr_parent_stance)
histogram(med_impact_curr_parent_stance)
histogram(no_impact_curr_parent_stance)

"""### Word distribution"""



"""# Naive Bayes"""

def clean_data(context, text):
    # remove root (topic)
    clean_context = [sents[1:] for sents in context]
    # append text to context
    clean_text = [clean_context[i] + [text[i]] for i in range(len(text))]
    clean_text = [' '.join([sent.lower() for sent in sents]) for sents in clean_text]

    return clean_text

train_text_input = clean_data(train_contexts, train_texts)
test_text_input = clean_data(test_contexts, test_texts)

train_text_input[0]

train_overall_stances[0]

train_contexts_impact[0]

"""## Feature Extraction"""

# train_text_tokens = [tokenize(text) for text in train_text_input]
# test_text_tokens  = [tokenize(text) for text in test_text_input]

# train_text_stemmed = [[tokenize_and_stem(text) for text in texts] for texts in train_text_input]
# test_text_stemmed  = [[tokenize_and_stem(text) for text in texts] for texts in test_text_input]
train_text_stemmed = [tokenize_and_stem(text) for text in train_text_input]
test_text_stemmed  = [tokenize_and_stem(text) for text in test_text_input]

# train_text_stemmed_joined = [[tokens for tokens in sent] for texts in train_text_stemmed for sent in texts]

train_2_gram = [n_gram(tokens, 2) for tokens in train_text_stemmed]
train_3_gram = [n_gram(tokens, 3) for tokens in train_text_stemmed]
train_4_gram = [n_gram(tokens, 4) for tokens in train_text_stemmed]
test_2_gram  = [n_gram(tokens, 2) for tokens in test_text_stemmed]
test_3_gram  = [n_gram(tokens, 3) for tokens in test_text_stemmed]
test_4_gram  = [n_gram(tokens, 4) for tokens in test_text_stemmed]

train_text_feats = [filter_stopwords(tokens) for tokens in train_text_stemmed]
test_text_feats = [filter_stopwords(tokens) for tokens in test_text_stemmed]
# train_text_feats = train_text_stemmed
# test_text_feats = test_text_stemmed

feats_set = set()

# build a Counter for stemmed features, e.g., {"text": 2, "mine": 1}
stemmed_feat_cnt = Counter()

for feats in train_text_feats:
    stemmed_feat_cnt.update(feats)

# add those stem features which occurs more than 10 times into the feature set.
feats_set.update([f for f, cnt in stemmed_feat_cnt.items() if cnt > 10]) 
    

# build a Counter for 2-gram features
bi_gram_feat_cnt = Counter()
for feats in train_2_gram:
    bi_gram_feat_cnt.update(feats)

# add those 2-gram features which occurs more than 10 times into the feature set.
feats_set.update([f for f, cnt in bi_gram_feat_cnt.items() if cnt > 10]) 


# build a Counter for 3-gram features
tri_gram_feat_cnt = Counter()

for feats in train_3_gram:
    tri_gram_feat_cnt.update(feats)

# add those 3-gram features which occurs more than 10 times into the feature set.
feats_set.update([f for f, cnt in tri_gram_feat_cnt.items() if cnt > 10]) 


# first, build a Counter for 4-gram features
four_gram_feat_cnt = Counter()

for feats in train_4_gram:
    four_gram_feat_cnt.update(feats)
    
# add those 4-gram features which occurs more than 10 times into the feature set.
feats_set.update([f for f, cnt in four_gram_feat_cnt.items() if cnt > 10]) 


print("Size of features:", len(feats_set))

# build the feature dict mapping each feature to its index 
feats_dict = dict(zip(feats_set, range(len(feats_set))))

# build the feature list
train_feats = list()
for i in range(len(train_id)):
    # concatenate the stemmed token list and all n-gram list together
    train_feats.append(train_text_feats[i] + train_2_gram[i] + train_3_gram[i] + train_4_gram[i])
    # train_feats.append(train_text_feats[i] + train_2_gram[i])

test_feats = list()
for i in range(len(test_id)):
    # concatenate the stemmed token list and all n-gram list together
    test_feats.append(test_text_feats[i] + test_2_gram[i] + test_3_gram[i] + test_4_gram[i])
    # test_feats.append(test_text_feats[i] + test_2_gram[i])

train_feats_matrix = np.vstack([get_onehot_vector(f, feats_dict) for f in train_feats])
test_feats_matrix = np.vstack([get_onehot_vector(f, feats_dict) for f in test_feats])

train_feats_matrix.shape, test_feats_matrix.shape

"""Next, we create a sequence matrix for the stance labels"""

from sklearn.preprocessing import MinMaxScaler
from keras.preprocessing.sequence import pad_sequences

scaler = MinMaxScaler()
stance_dict = {"SUPPORT":1., "OPPOSE": 0.}

train_stance_len = np.array([len(l) for l in train_overall_stances])[:, None]
test_stance_len = np.array([len(l) for l in test_overall_stances])[:, None]

max_len = max(np.max(train_stance_len), np.max(test_stance_len))
print("maximum number of stances: ", max_len)

# train_stance_len = scaler.fit_transform(train_stance_len)
# test_stance_len = scaler.fit_transform(test_stance_len)

def create_stance_matrix(stance, length=max_len):
    arr = np.zeros((length*2))
    for i in range(len(stance)):
        if stance[i] == 1:
            arr[2*i] += 1
        else:
            arr[2*i+1] += 1
    return arr

train_stance_matrix = np.array([create_stance_matrix(l[::-1]) for l in train_overall_stances])
test_stance_matrix  = np.array([create_stance_matrix(l[::-1]) for l in test_overall_stances])

# train_stance_matrix = scaler.fit_transform(train_stance_seq)
# test_stance_matrix = scaler.fit_transform(test_stance_seq)

# train_stance_matrix = pad_sequences(train_stance_seq, maxlen=max_len, dtype='float', 
#                                 value=0, truncating='post', padding='post')
# test_stance_matrix = pad_sequences(test_stance_seq, maxlen=max_len, dtype='float', 
#                                 value=0, truncating='post', padding='post')

# train_stance_matrix = scaler.fit_transform(train_stance_matrix)
# test_stance_matrix = scaler.fit_transform(test_stance_matrix)

def create_context_impact_matrix(context_impact, length=max_len):
    arr = np.zeros(length)
    for i in range(len(context_impact)):
        arr[i] = context_impact[i]
    return arr

train_contexts_impact_matrix = np.array([create_context_impact_matrix(l[::-1]) for l in train_contexts_impact])
test_contexts_impact_matrix = np.array([create_context_impact_matrix(l[::-1]) for l in test_contexts_impact])

# train_text_len = np.array([len(t) for t in train_text_feats])[:, None]
# test_text_len = np.array([len(t) for t in test_text_feats])[:, None]

# train_text_len = scaler.fit_transform(train_text_len)
# test_text_len = scaler.fit_transform(test_text_len)

# train_feats_matrix = np.hstack((train_feats_matrix, train_stance_matrix, train_stance_len, train_text_len))
# test_feats_matrix = np.hstack((test_feats_matrix, test_stance_matrix, test_stance_len, test_text_len))

train_feats_matrix = np.hstack((train_feats_matrix, train_stance_matrix, train_contexts_impact_matrix))
test_feats_matrix = np.hstack((test_feats_matrix, test_stance_matrix, test_contexts_impact_matrix))

train_feats_matrix.shape, test_feats_matrix.shape

train_feats_matrix = coo_matrix(train_feats_matrix)
test_feats_matrix = coo_matrix(test_feats_matrix)

"""## Training"""

from sklearn.naive_bayes import ComplementNB, GaussianNB

train_labels = np.array(train_labels)

n_fold = 5
np.random.seed(0)
# create the n-fold generator
skf = StratifiedKFold(n_fold, shuffle=True)

clfs = list()
valid_acc_list = list()
for k, (train_idx, valid_idx) in enumerate(
    skf.split(train_feats_matrix.toarray(), train_labels)):
    print("======Fold {}/{}======".format(k+1, n_fold))
    
    # build the classifier and train
    # clf = RandomForestClassifier()
    clf = ComplementNB()

    clf.fit(train_feats_matrix.toarray()[train_idx], train_labels[train_idx])
    
    #Get the predictions of the classifier
    train_pred = clf.predict(train_feats_matrix.toarray()[train_idx])
    valid_pred = clf.predict(train_feats_matrix.toarray()[valid_idx])

    #Compute accuracy scores
    train_score = f1_score(train_labels[train_idx], train_pred, average='macro')
    valid_score = f1_score(train_labels[valid_idx], valid_pred, average='macro')
    
    print("training f1 score: {}".format(train_score))
    print("validation f1 score: {}\n".format(valid_score))
    
    clfs.append(clf)
    valid_acc_list.append(valid_score)
    
print("\nAverage validation f1score: {}".format(sum(valid_acc_list)/len(valid_acc_list)))

"""## Evaluating"""

# get feature size
print("stemmed feature size:", len(stemmed_feat_cnt))

# then, get the sorted features by the frequency
stemmed_feat_keys = [f for f,cnt in stemmed_feat_cnt.most_common()]

# draw linear lines and log lines for sorted features
# set the figure size
plt.figure(figsize=(10,4))
# generate two subfigures and set current as the first one
plt.subplot(1,2,1)
# draw linear lines
plt.plot(range(1, len(stemmed_feat_cnt)+1), 
        [stemmed_feat_cnt[f] for f in stemmed_feat_keys])
# set labels
plt.xlabel("Feature Index")
plt.ylabel("Feature Frequency")
# set current as the second one
plt.subplot(1,2,2)
# draw log lines
plt.loglog(range(1, len(stemmed_feat_cnt)+1), 
           [stemmed_feat_cnt[f] for f in stemmed_feat_keys],
           basex=10, basey=10)
# set labels
plt.xlabel("Feature Index")
plt.ylabel("Feature Frequency")
plt.show()

"""## Testing"""

test_prob = None
for i, clf in enumerate(clfs):
    if(i==0):
        test_prob = clf.predict_proba(test_feats_matrix.toarray())
    else:
        test_prob += clf.predict_proba(test_feats_matrix.toarray())
        
test_pred = test_prob.argmax(axis=1)

"""# RNN

You can refer to this notebook for RNN:
https://colab.research.google.com/drive/11FE4NvPax8lXv9RyhRW_Nc6ioJocoXRv#scrollTo=W1MqpxUxq7b4

## Architecture
"""

def build_RNN(input_length, vocab_size, embedding_size,
              hidden_size, output_size,
              num_rnn_layers, num_mlp_layers,
              rnn_type="lstm",
              bidirectional=False,
              activation="relu",
              dropout_rate=0.5,
              batch_norm=False,
              l2_reg=0.01,
              loss="categorical_crossentropy",
              optimizer="Adam",
              learning_rate=0.001,
              metric="accuracy"):
    """
    :param input_length: the maximum length of sentences, type: int
    :param vocab_size: the vacabulary size, type: int
    :param embedding_size: the dimension of word representations, type: int
    :param hidden_size: the dimension of the hidden states, type: int
    :param output_size: the dimension of the prediction, type: int
    :param num_rnn_layers: the number of layers of the RNN, type: int
    :param num_mlp_layers: the number of layers of the MLP, type: int
    :param rnn_type: the type of RNN, type: str
    :param bidirectional: whether to use bidirectional rnn, type: bool
    :param activation: the activation type, type: str
    :param dropout_rate: the probability of dropout, type: float
    :param batch_norm: whether to enable batch normalization, type: bool
    :param l2_reg: the weight for the L2 regularizer, type: str
    :param loss: the training loss, type: str
    :param optimizer: the optimizer, type: str
    :param learning_rate: the learning rate for the optimizer, type: float
    :param metric: the metric, type: str
    return a RNN for text classification,
    # activation document: https://keras.io/activations/
    # dropout document: https://keras.io/layers/core/#dropout
    # embedding document: https://keras.io/layers/embeddings/#embedding
    # recurrent layers document: https://keras.io/layers/recurrent
    # batch normalization document: https://keras.io/layers/normalization/
    # losses document: https://keras.io/losses/
    # optimizers document: https://keras.io/optimizers/
    # metrics document: https://keras.io/metrics/
    """
    x = Input(shape=(input_length,))
    
    ################################
    ###### Word Representation #####
    ################################
    # word representation layer
    emb = Embedding(input_dim=vocab_size,
                    output_dim=embedding_size,
                    input_length=input_length,
                    embeddings_initializer=keras.initializers.TruncatedNormal(mean=0.0, stddev=0.1, seed=0))(x)
    
    ################################
    ####### Recurrent Layers #######
    ################################
    # recurrent layers
    # Referennce: https://keras.io/api/layers/#recurrent-layers
    if rnn_type == "rnn":
        fn = SimpleRNN
    elif rnn_type == "lstm":
        fn = LSTM
    elif rnn_type == "gru":
        fn = GRU
    else:
        raise NotImplementedError
        
    h = emb
    for i in range(num_rnn_layers):
        is_last = (i == num_rnn_layers-1)
        if bidirectional:
            h = Bidirectional(fn(hidden_size,
                   kernel_initializer=keras.initializers.glorot_uniform(seed=0),
                   recurrent_initializer=keras.initializers.Orthogonal(gain=1.0, seed=0),
                   return_sequences=not is_last))(h)
            # return_sequences:
            # Boolean. Whether to return the last output. in the output sequence, or the full sequence.
            # [h_1, h_2, ..., h_n] or h_n
        else:
            h = fn(hidden_size,
                   kernel_initializer=keras.initializers.glorot_uniform(seed=0),
                   recurrent_initializer=keras.initializers.Orthogonal(gain=1.0, seed=0),
                   return_sequences=not is_last)(h)
        h = Dropout(dropout_rate, seed=0)(h)
    
    ################################
    #### Fully Connected Layers ####
    ################################
    # multi-layer perceptron
    for i in range(num_mlp_layers-1):
        new_h = Dense(hidden_size,
                      kernel_initializer=keras.initializers.he_normal(seed=0),
                      bias_initializer="zeros",
                      kernel_regularizer=keras.regularizers.l2(l2_reg))(h)
        # add batch normalization layer
        if batch_norm:
            new_h = BatchNormalization()(new_h)
        # add residual connection
        if i == 0:
            h = new_h
        else:
            h = Add()([h, new_h])
        # add activation
        h = Activation(activation)(h)
    y = Dense(output_size,
              activation="softmax",
              kernel_initializer=keras.initializers.he_normal(seed=0),
              bias_initializer="zeros")(h)
    
    # set the loss, the optimizer, and the metric
    if optimizer == "SGD":
        optimizer = keras.optimizers.SGD(lr=learning_rate)
    elif optimizer == "RMSprop":
        optmizer = keras.optimizers.RMSprop(learning_rate=learning_rate)
    elif optimizer == "Adam":
        optmizer = keras.optimizers.Adam(learning_rate=learning_rate)
    else:
        raise NotImplementedError
    model = Model(x, y)
    model.compile(loss=loss, optimizer=optimizer, metrics=[metric])
    
    return model

"""## Training"""

seed(0)
tf.random.set_seed(0)
embedding_size = 50
hidden_size = 100 
num_rnn_layers = 3
num_mlp_layers = 3
os.makedirs("models", exist_ok=True)
model = build_RNN(max_len, len(feats_dict), embedding_size,
              hidden_size, num_classes,
              num_rnn_layers, num_mlp_layers,
              rnn_type="gru",
              bidirectional=True,
              activation="tanh",
              dropout_rate=0.5,
              batch_norm=True,
              l2_reg=0.05,
              loss="categorical_crossentropy",
              optimizer="Adam",
              learning_rate=0.01,
              metric="accuracy")

checkpointer = keras.callbacks.ModelCheckpoint(
    filepath=os.path.join("models", "rnn2_weights.hdf5"),
    monitor="val_accuracy",
    verbose=0,
    save_best_only=True)


history = model.fit(train_feats_matrix, train_label_matrix,
                    validation_split=0.1,
                    epochs=120, batch_size=100, verbose=0,
                    callbacks=[checkpointer])
model = keras.models.load_model(os.path.join("models", "rnn2_weights.hdf5"))

train_score = model.evaluate(train_feats_matrix, train_label_matrix,
                             batch_size=100)
test_score = model.evaluate(valid_feats_matrix, valid_label_matrix,
                            batch_size=100)
print("training loss:", train_score[0], "training accuracy", train_score[1])
print("test loss:", test_score[0], "test accuracy", test_score[1])

plt.figure(figsize=(10,4))
plt.subplot(1,2,1)
plt.plot(history.history["loss"], label="training", color="blue", linestyle="dashed")
plt.plot(history.history["val_loss"], label="validation", color="orange")
plt.xlabel("Iteration")
plt.ylabel("Loss")
plt.legend()
plt.subplot(1,2,2)
plt.plot(history.history["accuracy"], label="training", color="blue", linestyle="dashed")
plt.plot(history.history["val_accuracy"], label="validation", color="orange")
plt.xlabel("Iteration")
plt.ylabel("Accuracy")
plt.legend()
plt.show()

"""## Evaluation"""

train_pred = model.predict(train_feats_matrix)
train_pred = np.argmax(train_pred, axis=1)

valid_pred = model.predict(valid_feats_matrix)
valid_pred = np.argmax(valid_pred, axis=1)

train_acc = f1_score(np.array(train_labels), train_pred, average='macro')
valid_acc = f1_score(valid_labels, valid_pred, average='macro')

print('training accuracy: {}, validation accuracy: {}'.format(train_acc, valid_acc))

"""# CNN

## Feature Extraction
"""

train_text_input = [train_text_parents[i] + ' ' + train_texts[i].lower() for i in range(len(train_texts))]
test_text_input = [test_text_parents[i] + ' ' + test_texts[i].lower() for i in range(len(test_texts))]

def get_feats_dict(feats, min_freq=-1, max_freq=-1, max_size=-1):
    """
    :param data: a list of features, type: list(list)
    :param min_freq: the lowest fequency that the fequency of a feature smaller than it will be filtered out, type: int
    :param max_freq: the highest fequency that the fequency of a feature larger than it will be filtered out, type: int
    :param max_size: the max size of feature dict, type: int
    return a feature dict that maps features to indices, sorted by frequencies
    # Counter document: https://docs.python.org/3.6/library/collections.html#collections.Counter
    """
    # count all features
    feat_cnt = Counter(feats) # ["text", "text", "mine"] --> {"text": 2, "mine": 1}
    if max_size > 0 and min_freq == -1 and max_freq == -1:
        valid_feats = ["<pad>", "<unk>"] + [f for f, cnt in feat_cnt.most_common(max_size-2)]
    else:
        valid_feats = ["<pad>", "<unk>"]
        for f, cnt in feat_cnt.most_common():
            if (min_freq == -1 or cnt >= min_freq) and (max_freq == -1 or cnt <= max_freq):
                valid_feats.append(f)
    if max_size > 0 and len(valid_feats) > max_size:
        valid_feats = valid_feats[:max_size]
    print("Size of features:", len(valid_feats))
    
    # build a mapping from features to indices
    feats_dict = dict(zip(valid_feats, range(len(valid_feats))))
    return feats_dict

def get_index_vector(feats, feats_dict, max_len):
    """
    :param feats: a list of features, type: list
    :param feats_dict: a dict from features to indices, type: dict
    :param feats: a list of features, type: list
    return a feature vector,
    """
    # initialize the vector as all zeros
    vector = np.zeros(max_len, dtype=np.int64)
    for i, f in enumerate(feats):
        if i == max_len:
            break
        # get the feature index, return 1 (<unk>) if the feature is not existed
        f_idx = feats_dict.get(f, 1)
        vector[i] = f_idx
    return vector

# extract features
min_freq = 3
train_tokens = [tokenize(text) for text in train_text_input]
# valid_tokens = [tokenize(text) for text in valid_joined_texts]
test_tokens  = [tokenize(text) for text in test_text_input]

train_stemmed = [stem(tokens) for tokens in train_tokens]
# valid_stemmed = [stem(tokens) for tokens in valid_tokens]
test_stemmed  = [stem(tokens) for tokens in test_tokens]

train_feats = [filter_stopwords(tokens) for tokens in train_stemmed]
# valid_feats = [filter_stopwords(tokens) for tokens in valid_stemmed]
test_feats  = [filter_stopwords(tokens) for tokens in test_stemmed]

# build a mapping from features to indices
feats_dict = get_feats_dict(
    chain.from_iterable(train_feats),
    min_freq=min_freq)

plt.figure(figsize=(10,4))
len_cnt = Counter([len(feats) for feats in train_feats])
len_key = sorted(len_cnt)
plt.subplot(1,2,1)
plt.plot(range(1, len(len_key)+1),
         [len_cnt[l] for l in len_key])
plt.xlabel("Sentence Length")
plt.ylabel("Length Frequency")

plt.subplot(1,2,2)
plt.plot(range(1, len(len_key)+1),
         np.cumsum([len_cnt[l] for l in len_key]))
plt.xlabel("Sentence Length")
plt.ylabel("Cumulative Numbers")

max_len = 70

# build the feats_matrix
# convert each example to a index vector, and then stack vectors as a matrix
train_feats_matrix = np.vstack(
    [get_index_vector(f, feats_dict, max_len) for f in train_feats])
# valid_feats_matrix = np.vstack(
#     [get_index_vector(f, feats_dict, max_len) for f in valid_feats])
test_feats_matrix = np.vstack(
    [get_index_vector(f, feats_dict, max_len) for f in test_feats])

# convert labels to label_matrix
num_classes = max(train_labels) + 1
# convert each label to a ont-hot vector, and then stack vectors as a matrix
train_label_matrix = keras.utils.to_categorical(train_labels, num_classes=num_classes)
# valid_label_matrix = keras.utils.to_categorical(valid_labels, num_classes=num_classes)

train_text_stance = np.array([1 if stances[-1] == 'SUPPORT' else 0 for stances in train_stance_labels])[:, None]
test_text_stance = np.array([1 if stances[-1] == 'SUPPORT' else 0 for stances in test_stance_labels])[:, None]

train_parent_impact = np.array([l[-1] if len(l) >= 1 else 0 for l in train_contexts_impact])[:, None]
test_parent_impact = np.array([l[-1] if len(l) >= 1 else 0 for l in test_contexts_impact])[:, None]

train_feats_matrix = np.hstack((train_feats_matrix, train_text_stance, train_parent_impact))
test_feats_matrix = np.hstack((test_feats_matrix, test_text_stance, test_parent_impact))

"""## Architecture """

def build_CNN(input_length, vocab_size, embedding_size,
              hidden_size, output_size,
              kernel_sizes, num_filters, num_mlp_layers,
              padding="valid",
              strides=1,
              activation="relu",
              dropout_rate=0.0,
              batch_norm=False,
              l2_reg=0.0,
              loss="categorical_crossentropy",
              optimizer="Adam",
              learning_rate=0.1,
              metric="accuracy"):
    """
    :param input_length: the maximum length of sentences, type: int
    :param vocab_size: the vacabulary size, type: int
    :param embedding_size: the dimension of word representations, type: int
    :param hidden_size: the dimension of the hidden states, type: int
    :param output_size: the dimension of the prediction, type: int
    :param kernel_sizes: the kernel sizes of convolutional layers, type: list
    :param num_filters: the number of filters for each kernel, type: int
    :param num_mlp_layers: the number of layers of the MLP, type: int
    :param padding: the padding method in convolutional layers, type: str
    :param strides: the strides in convolutional layers, type: int
    :param activation: the activation type, type: str
    :param dropout_rate: the probability of dropout, type: float
    :param batch_norm: whether to enable batch normalization, type: bool
    :param l2_reg: the weight for the L2 regularizer, type: str
    :param loss: the training loss, type: str
    :param optimizer: the optimizer, type: str
    :param learning_rate: the learning rate for the optimizer, type: float
    :param metric: the metric, type: str
    return a CNN for text classification,
    # activation document: https://keras.io/activations/
    # dropout document: https://keras.io/layers/core/#dropout
    # embedding document: https://keras.io/layers/embeddings/#embedding
    # convolutional layers document: https://keras.io/layers/convolutional
    # pooling layers document: https://keras.io/layers/pooling/
    # batch normalization document: https://keras.io/layers/normalization/
    # losses document: https://keras.io/losses/
    # optimizers document: https://keras.io/optimizers/
    # metrics document: https://keras.io/metrics/
    """
    x = Input(shape=(input_length,))
    
    ################################
    ###### Word Representation #####
    ################################
    # word representation layer
    emb = Embedding(input_dim=vocab_size,
                    output_dim=embedding_size,
                    input_length=input_length,
                    embeddings_initializer=keras.initializers.TruncatedNormal(mean=0.0, stddev=0.1, seed=0))(x)
    
    ################################
    ########### Conv-Pool ##########
    ################################
    # convolutional and pooling layers
    cnn_results = list()
    for kernel_size in kernel_sizes:
        # add convolutional layer
        conv = Conv1D(filters=num_filters,
                      kernel_size=(kernel_size,),
                      padding=padding,
                      strides=strides)(emb)
        # add batch normalization layer
        if batch_norm:
            conv = BatchNormalization()(conv)
        # add activation
        conv = Activation(activation)(conv)
        # add max-pooling
        maxpool = MaxPool1D(pool_size=(input_length-kernel_size)//strides+1)(conv)
        cnn_results.append(Flatten()(maxpool))
    
    ################################
    ##### Fully Connected Layer ####
    ################################
    h = Concatenate()(cnn_results) if len(kernel_sizes) > 1 else cnn_results[0]
    h = Dropout(dropout_rate, seed=0)(h)
    # multi-layer perceptron
    for i in range(num_mlp_layers-1):
        new_h = Dense(hidden_size,
                      kernel_regularizer=keras.regularizers.l2(l2_reg))(h)
        # add batch normalization layer
        if batch_norm:
            new_h = BatchNormalization()(new_h)
        # add skip connection
        if i == 0:
            h = new_h
        else:
            h = Add()([h, new_h])
        # add activation
        h = Activation(activation)(h)
    y = Dense(output_size,
              activation="softmax")(h)
    
    # set the loss, the optimizer, and the metric
    if optimizer == "SGD":
        optimizer = keras.optimizers.SGD(lr=learning_rate)
    elif optimizer == "RMSprop":
        optmizer = keras.optimizers.RMSprop(learning_rate=learning_rate)
    elif optimizer == "Adam":
        optmizer = keras.optimizers.Adam(learning_rate=learning_rate)
    else:
        raise NotImplementedError
    model = Model(x, y)
    model.compile(loss=loss, optimizer=optimizer, metrics=[metric])
    
    return model

"""## Training"""

seed(0)
tf.random.set_seed(0)

model = build_CNN(input_length=len(train_feats_matrix[0]), vocab_size=len(feats_dict),
                  embedding_size=100, hidden_size=100, output_size=num_classes,
                  kernel_sizes=[1,2,3,4], num_filters=100, num_mlp_layers=2,
                  activation="relu",
                  dropout_rate=0.5, l2_reg=0.1, learning_rate=0.005, batch_norm=True)
checkpointer = keras.callbacks.ModelCheckpoint(
    filepath=os.path.join("models", "cnn2_weights.hdf5"),
    monitor="val_accuracy",
    verbose=1,
    save_best_only=True)


history = model.fit(train_feats_matrix, train_label_matrix,
                    validation_split=0.2,
                    epochs=50, batch_size=50, verbose=0,
                    callbacks=[checkpointer])

"""## Evaluation"""

plt.figure(figsize=(10,4))
plt.subplot(1,2,1)
plt.plot(history.history["loss"], label="training", color="blue", linestyle="dashed")
plt.plot(history.history["val_loss"], label="validation", color="orange")
plt.xlabel("Iteration")
plt.ylabel("Loss")
plt.legend()
plt.subplot(1,2,2)
plt.plot(history.history["accuracy"], label="training", color="blue", linestyle="dashed")
plt.plot(history.history["val_accuracy"], label="validation", color="orange")
plt.xlabel("Iteration")
plt.ylabel("Accuracy")
plt.legend()
plt.show()

model = keras.models.load_model(os.path.join("models", "cnn2_weights.hdf5"))

train_score = model.evaluate(train_feats_matrix, train_label_matrix, batch_size=50)
print("training loss:", train_score[0], "training accuracy", train_score[1])

train_pred = model.predict(train_feats_matrix)
train_pred = np.argmax(train_pred, axis=1)
train_score = f1_score(np.array(train_labels), train_pred, average='macro')

print('training macro f1 score: {}'.format(train_score))

test_pred = model.predict(test_feats_matrix)
test_pred = np.argmax(test_pred, axis=1)

"""# BERT"""

tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)
# tokenizer.add_special_tokens(['[NULL]', '[OPPOSE]', '[SUPPORT]'])
# tokenizer.add_tokens(['null', 'oppose', 'support'])

"""## Data Prep

### Clean Data
"""

def combine_stance(text, context, stance):
    combined_context = []
    for i in range(len(stance)-1):
        combined_context.append(stance[i].lower() + ' ' + remove_stop_words(context[i].lower()))
    combined_text = stance[-1].lower() + ' ' + remove_stop_words(text.lower())
    return combined_text, ' '.join(combined_context)

def get_input(texts, contexts, stances):
    texts_output, contexts_output = [], []
    for i in range(len(texts)):
        text, context = combine_stance(texts[i], contexts[i], stances[i])
        texts_output.append(text)
        contexts_output.append(context)
    return texts_output, contexts_output

train_texts, train_contexts = get_input(train_texts, train_contexts, train_stance_labels)
valid_texts, valid_contexts = get_input(valid_texts, valid_contexts, valid_stance_labels)
test_texts,  test_contexts  = get_input(test_texts, test_contexts, test_stance_labels)

"""### DataLoader
To save memory consumption during training, we need to create PyTorch DataLoader with custom data member class so that we can use it to feed into the BERT model. To create such custom data that is compatible with PyTorch, we need to implement the `__len__` and `__getitem__` as referred here (https://pytorch.org/tutorials/beginner/data_loading_tutorial.html)
"""

class Bert_data(Dataset):
    def __init__(self, tokenizer, texts, contexts, labels):
        self.tokenizer = tokenizer
        self.texts = texts
        self.contexts = contexts
        self.labels = labels
        self.max_length = 300

    def __len__(self):
        return len(self.labels)

    def __getitem__(self, idx):
        text = self.texts[idx]
        context = self.contexts[idx]
        label = self.labels[idx]

        encoding = self.tokenizer.encode_plus(text, context, padding='max_length',
                                              truncation=True, max_length=self.max_length,
                                              return_attention_mask=True, return_tensors='pt')
        attention_mask = encoding['attention_mask'].flatten()
        input_ids = encoding['input_ids'].flatten()

        sample = {'label': label,
                  'attention_mask': attention_mask,
                  'input_ids': input_ids
                  }

        return sample

train_dataloader = DataLoader(Bert_data(tokenizer, train_texts, train_contexts, train_labels), batch_size=32, num_workers=2)
valid_dataloader = DataLoader(Bert_data(tokenizer, valid_texts, valid_contexts, valid_labels), batch_size=32, num_workers=2)
test_dataloader  = DataLoader(Bert_data(tokenizer, test_texts, test_contexts, test_labels), batch_size=32, num_workers=2)

"""### Input to IDs

We need to first convert textual data into data that BERT can recognize by tokenizing the words as we have learned in lectures. However, we also want to incorporate the stance and sequential ordering into consideration. Therefore, we can create special tokens for these stances.
"""

tokenizer.encode("[NULL] topic [OPPOSE] argument", add_special_tokens=True).ids

def transform_input(texts, stances):
    """
    texts: list(str)
        a list containing [context] + [text], i.e. ['topic', 'arg1', 'arg2', ..., 'text']
    stances: list(str)
        a list containing stances that corrsponds to each sentence in texts
    return: str
    """
    result = ""
    for i in range(len(stances)):
        result += "[" + stances[i] + "]" # create special token
        result += ' '
        result += texts[i] # add the sentence corresponding to the stance
        result += ' '
    return result

a = transform_input(["topic", "argument"], ["NULL", "OPPOSE"])
tokenizer.encode(a, add_special_tokens=True).ids

def get_list_ids(texts, contexts, stances):
    """
    texts: list(str)
    contexts: list(list(str))
    stances: list(list(str))
    return: list(list(int))
    """
    input_ids = []
    for i in range(len(texts)):
        combined_text = transform_input(contexts[i]+[texts[i]], stances[i])
        encoded_input = tokenizer.encode(combined_text)
        input_ids.append(encoded_input.ids)
    return input_ids

train_input_ids = get_list_ids(train_texts, train_contexts, train_stance_labels)
valid_input_ids = get_list_ids(valid_texts, valid_contexts, valid_stance_labels)
test_input_ids = get_list_ids(test_texts, test_contexts, test_stance_labels)

len(train_texts)

len(valid_texts)

"""### Padding and Truncating

Since BERT only accepts maximum sentence length of 512 tokens, and all sentences mus be padded or truncated to a single, fixed length. We must padd and truncate into a specific size
"""

def tokens_stat(tokens):
    
    lengths = [len(ids) for ids in tokens]
    plt.hist(lengths)
    plt.show()
    print('max sentence length: {}, min sentence length: {}'.format(np.max(lengths), np.min(lengths)))
    print('avg sentence length: {}, std: {}'.format(np.mean(lengths), np.std(lengths)))

tokens_stat(train_input_ids)
tokens_stat(valid_input_ids)
tokens_stat(test_input_ids)

"""It looks like there are several outliers that exceeds the BERT 512 tokens limit, while the majority is under 200 tokens in length. 

We can set the maximum to 512 tokens in order to preserve most of the data. Now, we need to truncate and pad the rest to match 512 tokens
"""

from keras.preprocessing.sequence import pad_sequences

max_length = 200

train_input_matrix = pad_sequences(train_input_ids, maxlen=max_length, dtype='long', 
                                   value=0, truncating='pre', padding='post')
valid_input_matrix = pad_sequences(valid_input_ids, maxlen=max_length, dtype='long', 
                                   value=0, truncating='pre', padding='post')
test_input_matrix = pad_sequences(test_input_ids, maxlen=max_length, dtype='long', 
                                   value=0, truncating='pre', padding='post')

tokenizer.decode(train_input_ids[0]) == tokenizer.decode(train_input_matrix[0])

"""### Attention Masks

We do not want BERT to be concern with any of the pad tokens, so we create a mask for it
"""

def get_attention_mask(label_matrix):
    """
    
    """
    result = []
    for ids in label_matrix:
        arr = [int(token > 0) for token in ids]
        result.append(arr)
    return result

train_attention_mask = get_attention_mask(train_input_matrix)
valid_attention_mask = get_attention_mask(valid_input_matrix)
test_attention_mask = get_attention_mask(test_input_matrix)

"""### Convert to Tensor

After gathering the features, we now need to prepare the feature matrix and mask into Tensors which can be feed into the BERT models
"""

train_inputs = torch.tensor(train_input_matrix)
valid_inputs = torch.tensor(valid_input_matrix)
test_inputs = torch.tensor(test_input_matrix)

train_masks = torch.tensor(train_attention_mask)
valid_masks = torch.tensor(valid_attention_mask)
test_masks = torch.tensor(test_attention_mask)

train_labels = torch.tensor(train_labels)
valid_labels = torch.tensor(valid_labels)
test_labels = torch.tensor(test_labels)

from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler
batch_size = 32

train_data = TensorDataset(train_inputs, train_masks, train_labels)
train_sampler = RandomSampler(train_data)
train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)

valid_data = TensorDataset(valid_inputs, valid_masks, valid_labels)
valid_sampler = SequentialSampler(valid_data)
valid_dataloader = DataLoader(valid_data, sampler=valid_sampler, batch_size=batch_size)

test_data = TensorDataset(test_inputs, test_masks, test_labels)
test_sampler = SequentialSampler(test_data)
test_dataloader = DataLoader(test_data, sampler=test_sampler, batch_size=batch_size)

"""## Model"""

class Bert_model(nn.Module):
    def __init__(self, pretrained="bert-base-uncased", hidden_size=32, num_class=3):
        super(Bert_model, self).__init__()
        self.pretrained = BertModel.from_pretrained(pretrained)
        self.dropout = nn.Dropout(0.5)
        self.fc = nn.Sequential(
            nn.Linear(self.pretrained.config.hidden_size, hidden_size),
            nn.BatchNorm1d(hidden_size),
            nn.ReLU(),
            nn.Linear(hidden_size, num_class)
        )
    
    def forward(self, ids, mask):
        x = self.pretrained(input_ids=ids, attention_mask=mask)
        x = self.dropout(x['last_hidden_state'][:,0])
        return self.fc(x)

def eval(model, valid_data, loss_function):
    model.eval()
    valid_score = 0
    loss = 0
    with torch.no_grad():
        for i, batch in enumerate(valid_data):
            batch_ids = batch['input_ids'].to(device)
            batch_masks = batch['attention_mask'].to(device)
            batch_labels = batch['label'].to(device)

            output = model(ids=batch_ids, mask=batch_masks)
            _, pred = torch.max(output, dim=1)
            loss += loss_function(output, batch_labels)

            preds = pred.to('cpu').numpy()
            labels = batch_labels.to('cpu').numpy().flatten()

            valid_score += f1_score(preds, labels, average='macro')

    print("validation f1 score: {}, validation loss: {}".format(valid_score/len(valid_data), 
                                                                loss/len(valid_data)))

def train(model, params, train_data, valid_data):
    optim = AdamW(model.parameters(), lr=params['lr'], eps=params['eps'], correct_bias=False)
    scheduler = get_linear_schedule_with_warmup(optim, 
                                                num_warmup_steps=0, 
                                                num_training_steps=len(train_data)*params['epoch'])
    loss_function = nn.CrossEntropyLoss().to(device)
    losses = []
    for i in range(0, params['epoch']):
        print("Epoch {}/{}".format(i, params['epoch']))
        model.train()
        epoch_loss = 0
        for i, batch in enumerate(train_data):
            # model.zero_grad()

            batch_ids = batch['input_ids'].to(device)
            batch_masks = batch['attention_mask'].to(device)
            batch_labels = batch['label'].to(device)

            output = model(ids=batch_ids, mask=batch_masks)
            _, pred = torch.max(output, dim=1)

            loss = loss_function(output, batch_labels)
            losses.append(loss)
            epoch_loss += loss

            loss.backward()
            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)
            optim.step()
            scheduler.step()
            optim.zero_grad()

        print("Epoch average loss: {}".format(epoch_loss/len(train_data)))
        eval(model, valid_data, loss_function)
        model.train()
    
    return losses

"""## Training"""

model = Bert_model().to(device)

params = {
    'lr': 1e-5,
    'eps': 1e-8,
    'epoch': 6
}

losses = train(model, params, train_dataloader, valid_dataloader)

test_pred = []
with torch.no_grad():
    for i, batch in enumerate(test_dataloader):
        batch_ids = batch['input_ids'].to(device)
        batch_masks = batch['attention_mask'].to(device)

        output = model(ids=batch_ids, mask=batch_masks)
        _, pred = torch.max(output, dim=1)
        test_pred.extend(pred.to('cpu').numpy())

"""## Output prediction to kaggle"""

def write_predictions(file_name, ids, pred):
    df = pd.DataFrame(zip(ids, pred))
    df.columns = ["id", "pred"]
    df.to_csv(file_name, index=False)
    return df

sub_df = write_predictions("submission_cnn.csv", test_id, test_pred)

sub_df['pred'].value_counts(normalize=1)

train_df['impact_label'].value_counts(normalize=1)