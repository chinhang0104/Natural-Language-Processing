# -*- coding: utf-8 -*-
"""MSBD6000H Project 2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Z0NYRKclZIy8WB6NgAqb-NAKId12JnRQ

# Guide
Kaggle: https://www.kaggle.com/c/ml4nlp-covid19ner/ \
playground.ipynb: https://www.kaggle.com/tianqingfang/playground \

# Import library and load data
"""

from google.colab import drive
drive.mount('/content/gdrive')

!pwd

# Commented out IPython magic to ensure Python compatibility.
# %cd gdrive/MyDrive/NLP_Project2/

# Commented out IPython magic to ensure Python compatibility.
# %tensorflow_version 1.x
import keras
from keras.utils import to_categorical
import numpy as np
import os
import pickle as pkl
import pandas as pd

train_dict = pkl.load(open("train.pkl", "rb"))
val_dict = pkl.load(open("val.pkl", "rb"))
test_dict = pkl.load(open("test.pkl", "rb"))
print("keys in train_dict:", train_dict.keys())
print("keys in val_dict:", val_dict.keys())
print("keys in test_dict:", test_dict.keys())

"""# Word2vec (Done by Eric)

I used pretrained word embeddings (word2vec, Glove) for lower dimensional embedding. \

Each word use 300 dim vector to represcent, instead of a number in playground.ipynb. \

Output: train_tokens, train_tags, val_tokens, val_tags
"""

from gensim.models.word2vec import Word2Vec
import gensim.downloader as api

# Download the model and return as object ready for use. You can find more models on https://github.com/RaRe-Technologies/gensim-data
model = api.load('word2vec-google-news-300')  # Google News (about 100 billion words)
#model = api.load('glove-twitter-200')  # Twitter (2B tweets, 27B tokens, 1.2M vocab, uncased)
#model = api.load('glove-wiki-gigaword-300	')  # Wikipedia 2014 + Gigaword 5 (6B tokens, uncased)

# prepare word vocab and tag vocab

vocab_dict = {'_unk_': np.zeros(300), '_w_pad_': np.zeros(300)}

for doc in train_dict['word_seq']:
    for word in doc:
        if(word not in vocab_dict):
            if word in model:
                vocab_dict[word] = model[word]
            else:
                vocab_dict[word] = np.zeros(300)

tag_dict = {'_t_pad_': 0} # add a padding token

for tag_seq in train_dict['tag_seq']:
    for tag in tag_seq:
        if(tag not in tag_dict):
            tag_dict[tag] = len(tag_dict)
word2idx = vocab_dict
#idx2word = {v:k for k,v in word2idx.items()}
tag2idx = tag_dict
idx2tag = {v:k for k,v in tag2idx.items()}            

print("size of word vocab:", len(vocab_dict), "size of tag_dict:", len(tag_dict))

# The maximum length of a sentence is set to 128
max_sent_length = 128

train_tokens = np.array([[word2idx[w] for w in doc] for doc in train_dict['word_seq']])
val_tokens = np.array([[word2idx.get(w, 0) for w in doc] for doc in val_dict['word_seq']])
test_tokens = np.array([[word2idx.get(w, 0) for w in doc] for doc in test_dict['word_seq']])


train_tags = [[tag2idx[t] for t in t_seq] for t_seq in train_dict['tag_seq']]
train_tags = np.array([to_categorical(t_seq, num_classes=len(tag_dict)) for t_seq in train_tags])

val_tags = [[tag2idx[t] for t in t_seq] for t_seq in val_dict['tag_seq']]
val_tags = np.array([to_categorical(t_seq, num_classes=len(tag_dict)) for t_seq in val_tags])

# we don't have test tags

train_tokens

print("training size:", train_tokens.shape, "tag size:", train_tags.shape)
print("validating size:", val_tokens.shape, "tag size:", val_tags.shape)

#@title
from transformers import AutoTokenizer
    
tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)

"""# Base model using word2vec
Use pretrained word embeddings (word2vec, Glove). \
Use lower dimensional embedding, and CNN/RNN to get contextualized representations of tokens, plus a dense layer to predict the labels.
Use pretrained word embeddings (word2vec, Glove, etc, which you can find online), multiple layers of neural networks, bidirectional RNN/LSTM, etc.


"""

# Import the Keras libraries and packages
from keras.models import Sequential
from keras.layers import Dense
from keras.layers import LSTM
from keras.layers import Dropout
from keras.layers import Reshape
from keras.models import Sequential, Model
from keras.layers import Dense, Activation, Embedding, Dropout, BatchNormalization, Input, Add, Concatenate,\
    Bidirectional, SimpleRNN, LSTM, GRU, TimeDistributed
import tensorflow as tf
from keras.callbacks import ModelCheckpoint, Callback

#Failed to run
output_dim = 100
n_tags = 65
def build_RNN():
    model = Sequential()

    # Add Embedding layer
    #model.add(Embedding(input_dim=input_dim, output_dim=output_dim, input_length=input_length))
    model.add(keras.Input(shape=(128, 300)))

    # Add bidirectional LSTM
    model.add(Bidirectional(LSTM(units=output_dim, return_sequences=True, dropout=0.2, recurrent_dropout=0.2), merge_mode = 'concat'))

    # Add LSTM
    model.add(LSTM(units=output_dim, return_sequences=True, dropout=0.5, recurrent_dropout=0.5))

    # Add timeDistributed Layer
    model.add(TimeDistributed(Dense(n_tags, activation="relu")))

    #Optimiser 
    # adam = k.optimizers.Adam(lr=0.0005, beta_1=0.9, beta_2=0.999)

    # Compile model
    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])
    model.summary()
    
    return model

build_RNN().summary()

build_RNN().fit(train_tokens, train_tags,
                    validation_split=0.1,
                    epochs=100, batch_size=100, verbose=0,
                    callbacks=[checkpointer, earlystopping])

"""# CRF (Done by Cathy)

CRF layer is an optimisation on top of BI-LSTM layer. It can be used to predict the current tag based on the past attributed tags. 


"""

temp_train_df=pd.DataFrame.from_dict(train_dict)
print(temp_train_df)

train_df_seq=temp_train_df.apply(lambda x: pd.Series(x['word_seq']),axis=1).stack().reset_index(level=1,drop=True)
train_df_tag=temp_train_df.apply(lambda x: pd.Series(x['tag_seq']),axis=1).stack().reset_index(level=1,drop=True)

train_df= pd.concat([train_df_seq, train_df_tag], axis=1)
train_df = train_df.reset_index()
train_df.columns =['sentence_idx', 'word', 'tag']
train_df

temp_val_df=pd.DataFrame.from_dict(val_dict)
print(temp_val_df)

val_df_seq=temp_val_df.apply(lambda x: pd.Series(x['word_seq']),axis=1).stack().reset_index(level=1,drop=True)
val_df_tag=temp_val_df.apply(lambda x: pd.Series(x['tag_seq']),axis=1).stack().reset_index(level=1,drop=True)

val_df= pd.concat([val_df_seq, val_df_tag], axis=1)
val_df = val_df.reset_index()
val_df.columns =['sentence_idx', 'word', 'tag']
val_df

class SentenceGetter(object):
    
    def __init__(self, dataset):
        self.n_sent = 1
        self.dataset = dataset
        self.empty = False
        agg_func = lambda s: [(w, t) for w,t in zip(s["word"].values.tolist(),
                                                        s["tag"].values.tolist())]
        self.grouped = self.dataset.groupby("sentence_idx").apply(agg_func)
        self.sentences = [s for s in self.grouped]
    
    def get_next(self):
        try:
            s = self.grouped["Sentence: {}".format(self.n_sent)]
            self.n_sent += 1
            return s
        except:
            return None
          
getter = SentenceGetter(train_df)
sentences = getter.sentences

from math import nan

words = list(set(train_df["word"].values))
n_words = len(words)

tags = []
for tag in set(train_df["tag"].values):
    if tag is nan or isinstance(tag, float):
        tags.append('unk')
    else:
        tags.append(tag)
n_tags = len(tags)

from future.utils import iteritems

word2idx = {w: i for i, w in enumerate(words)}
tag2idx = {t: i for i, t in enumerate(tags)}
idx2tag = {v: k for k, v in iteritems(tag2idx)}

from keras.preprocessing.sequence import pad_sequences
from keras.utils import to_categorical
from sklearn.model_selection import train_test_split

maxlen = max([len(s) for s in sentences])

X = [[word2idx[w[0]] for w in s] for s in sentences]
X = pad_sequences(maxlen=maxlen, sequences=X, padding="post",value=n_words - 1)

y = [[tag2idx[w[1]] for w in s] for s in sentences]
y = pad_sequences(maxlen=maxlen, sequences=y, padding="post", value=tag2idx["O"])
y = [to_categorical(i, num_classes=n_tags) for i in y]

# Split train and test data
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)

pip install git+https://www.github.com/keras-team/keras-contrib.git

from keras.models import Model, Input
from keras.layers import LSTM, Embedding, Dense, TimeDistributed, Dropout, Bidirectional
import keras as k
from keras_contrib.layers import CRF
import tensorflow as tf
import tensorflow_hub as hub
from keras import backend as K

input = Input(shape=(128,))
word_embedding_size = 128

# Embedding Layer
model = Embedding(input_dim=n_words, output_dim=word_embedding_size, input_length=128)(input)

# BI-LSTM Layer
model = Bidirectional(LSTM(units=word_embedding_size, 
                           return_sequences=True, 
                           dropout=0.5, 
                           recurrent_dropout=0.5, 
                           kernel_initializer=k.initializers.he_normal()))(model)
model = LSTM(units=word_embedding_size * 2, 
             return_sequences=True, 
             dropout=0.5, 
             recurrent_dropout=0.5, 
             kernel_initializer=k.initializers.he_normal())(model)

# TimeDistributed Layer
model = TimeDistributed(Dense(n_tags, activation="relu"))(model)  

# CRF Layer
crf = CRF(n_tags)

out = crf(model)  # output
model = Model(input, out)

from keras.callbacks import ModelCheckpoint
import matplotlib.pyplot as plt

#Optimiser 
adam = k.optimizers.Adam(lr=0.0005, beta_1=0.9, beta_2=0.999)

# Compile model
model.compile(optimizer=adam, loss=crf.loss_function, metrics=[crf.accuracy, 'accuracy'])

model.summary()

# Saving the best model only
filepath="ner-bi-lstm-td-model-{val_accuracy:.2f}.hdf5"
checkpoint = ModelCheckpoint(filepath, monitor='val_accuracy', verbose=1, save_best_only=True, mode='max')
callbacks_list = [checkpoint]


# Fit the best model
history = model.fit(X, np.array(y), batch_size=128, epochs=10, validation_split=0.2, verbose=1, callbacks=callbacks_list)

"""# Output format

In this project, you should predict the NER tags for the test set tokens.

The index of test set starts from 0 and ends with 2949.

You should write the predictions into a .csv file, using the `get_pred_csv` function.

E.g.

|id|labels|
|:--:|:--:|
|0_0|`O`|
|0_1|`GENE_OR_GENOME`|
|...|...|
|1_0|`O`|
|1_1|`GENE_OR_GENOME`|
|...|...|

Format requirements:
1. The first column `id` should be of the format `{0:d}_{1:d}`, where the first element is the id of test set sentences, and the second element is the id of the tokens in the sentence.
2. The second column `labels` should be a string indicating the predicted NER tag.
"""

def get_pred_csv(pred, f_dict, padding_id="_w_pad_"):
    ids = [str(i)+"_"+str(w_id) for i, _ in enumerate(f_dict["id"]) for w_id, word in enumerate(f_dict["word_seq"][i]) if word != padding_id]
    tags = [ps[w_id] for i, ps in enumerate(pred) for w_id, word in enumerate(f_dict["word_seq"][i]) if word != padding_id]
    return {"id":ids, "tag":tags}
def get_csv_accuracy(d1, d2):
    assert len(d1["id"]) == len(d2["id"])
    return sum(np.array(d1["tag"]) == np.array(d2["tag"])) / len(d1["tag"])

test_preds = np.array([[idx2tag[p] for p in preds] for preds in np.ones((len(test_dict["id"]), max_sent_length))])

test_preds_dict = get_pred_csv(test_preds, test_dict)
pd.DataFrame(test_preds_dict).to_csv("test_pred.csv", index=False)