{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\chinh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\chinh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy of LR: 0.46\n",
      "P_y: [0.08678304 0.09426434 0.12169576 0.29625935 0.40099751]\n",
      "P_xy.shape: (3258, 5)\n",
      "Test Accuracy of NB: 0.485\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "from itertools import chain\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from scipy.sparse import coo_matrix\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import nltk\n",
    "nltk.download ('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "stopwords = set(stopwords.words(\"english\"))\n",
    "ps = PorterStemmer()\n",
    "np.random.seed(0)\n",
    "\n",
    "\n",
    "\n",
    "def load_data(file_name):\n",
    "    \"\"\"\n",
    "    :param file_name: a file name, type: str\n",
    "    return a list of ids, a list of reviews, a list of labels\n",
    "    https://pandas.pydata.org/pandas-docs/stable/generated/pandas.read_csv.html\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(file_name)\n",
    "\n",
    "    return df['id'], df[\"text\"], df['label']\n",
    "\n",
    "\n",
    "\n",
    "def load_labels(file_name):\n",
    "    \"\"\"\n",
    "    :param file_name: a file name, type: str\n",
    "    return a list of labels\n",
    "    \"\"\"\n",
    "    return pd.read_csv(file_name)['label']\n",
    "\n",
    "\n",
    "def tokenize(text):\n",
    "    \"\"\"\n",
    "    :param text: a doc with multiple sentences, type: str\n",
    "    return a word list, type: list\n",
    "    e.g.\n",
    "    Input: 'Text mining is to identify useful information.'\n",
    "    Output: ['Text', 'mining', 'is', 'to', 'identify', 'useful', 'information', '.']\n",
    "    \"\"\"\n",
    "    return nltk.word_tokenize(text)\n",
    "\n",
    "def stem(tokens):\n",
    "    \"\"\"\n",
    "    :param tokens: a list of tokens, type: list\n",
    "    return a list of stemmed words, type: list\n",
    "    e.g.\n",
    "    Input: ['Text', 'mining', 'is', 'to', 'identify', 'useful', 'information', '.']\n",
    "    Output: ['text', 'mine', 'is', 'to', 'identifi', 'use', 'inform', '.']\n",
    "    \"\"\"\n",
    "    ### equivalent code\n",
    "    # results = list()\n",
    "    # for token in tokens:\n",
    "    #     results.append(ps.stem(token))\n",
    "    # return results\n",
    "\n",
    "    return [ps.stem(token) for token in tokens]\n",
    "\n",
    "def n_gram(tokens, n=1):\n",
    "    \"\"\"\n",
    "    :param tokens: a list of tokens, type: list\n",
    "    :param n: the corresponding n-gram, type: int\n",
    "    return a list of n-gram tokens, type: list\n",
    "    e.g.\n",
    "    Input: ['text', 'mine', 'is', 'to', 'identifi', 'use', 'inform', '.'], 2\n",
    "    Output: ['text mine', 'mine is', 'is to', 'to identifi', 'identifi use', 'use inform', 'inform .']\n",
    "    \"\"\"\n",
    "    if n == 1:\n",
    "        return tokens\n",
    "    else:\n",
    "        results = list()\n",
    "        for i in range(len(tokens)-n+1):\n",
    "            # tokens[i:i+n] will return a sublist from i th to i+n th (i+n th is not included)\n",
    "            results.append(\" \".join(tokens[i:i+n]))\n",
    "        return results\n",
    "\n",
    "\n",
    "def filter_stopwords(tokens):\n",
    "    \"\"\"\n",
    "    :param tokens: a list of tokens, type: list\n",
    "    return a list of filtered tokens, type: list\n",
    "    e.g.\n",
    "    Input: ['text', 'mine', 'is', 'to', 'identifi', 'use', 'inform', '.']\n",
    "    Output: ['text', 'mine', 'identifi', 'use', 'inform', '.']\n",
    "    \"\"\"\n",
    "    ### equivalent code\n",
    "    # results = list()\n",
    "    # for token in tokens:\n",
    "    #     if token not in stopwords and not token.isnumeric():\n",
    "    #         results.append(token)\n",
    "    # return results\n",
    "\n",
    "    return [token for token in tokens if token not in stopwords and not token.isnumeric()]\n",
    "\n",
    "\n",
    "def get_onehot_vector(feats, feats_dict):\n",
    "    \"\"\"\n",
    "    :param data: a list of features, type: list\n",
    "    :param feats_dict: a dict from features to indices, type: dict\n",
    "    return a feature vector,\n",
    "    \"\"\"\n",
    "    # initialize the vector as all zeros\n",
    "    vector = np.zeros(len(feats_dict), dtype=np.float)\n",
    "    for f in feats:\n",
    "        # get the feature index, return -1 if the feature is not existed\n",
    "        f_idx = feats_dict.get(f, -1)\n",
    "        if f_idx != -1:\n",
    "            # set the corresponding element as 1\n",
    "            vector[f_idx] = 1\n",
    "    return vector\n",
    "\n",
    "def to_numerical(train_n_gram, test_n_gram, min_thresh=10):\n",
    "\n",
    "    # build the feature dict mapping each feature to its index \n",
    "    \n",
    "    ###### Your Code Here ######\n",
    "    # build a Counter for n-gram features\n",
    "    feat_cnter = Counter()\n",
    "    for feats in train_n_gram:\n",
    "        feat_cnter.update(feats)\n",
    "\n",
    "    # add those n-gram features which occurs more than min_thresh times into the feature set.\n",
    "    ngram_vocab = [f for f, cnt in feat_cnter.items() if cnt > min_thresh]\n",
    "\n",
    "    feats_dict = dict(zip(ngram_vocab, range(len(ngram_vocab))))\n",
    "    \n",
    "    # build the feature list\n",
    "    train_feats = list()\n",
    "    for i in range(len(train_ids)):\n",
    "    # concatenate the stemmed token list and all n-gram list together\n",
    "        train_feats.append(train_stemmed[i] + train_n_gram[i])\n",
    "    \n",
    "    test_feats = list()\n",
    "    \n",
    "    for i in range(len(test_ids)):\n",
    "        # concatenate the stemmed token list and all n-gram list together\n",
    "        test_feats.append(test_stemmed[i] + test_n_gram[i])\n",
    "    # build the feats_matrix for both train and test set.\n",
    "    # We first convert each example to a ont-hot vector, and then stack vectors as a matrix. Afterwards,\n",
    "    # we save this feature matirx in a COO sparse matrix format to reduce memory consumption.\n",
    "\n",
    "    train_feats_matrix = coo_matrix(np.vstack([get_onehot_vector(f, feats_dict) for f in train_feats]))\n",
    "    test_feats_matrix = coo_matrix(np.vstack([get_onehot_vector(f, feats_dict) for f in test_feats]))\n",
    "\n",
    "\n",
    "    return train_feats_matrix, test_feats_matrix\n",
    "\n",
    "def classify_lr(train_feats_matrix, test_feats_matrix, train_labels):\n",
    "    ###### Your Code Here ######\n",
    "    clf0 = LogisticRegression()\n",
    "    clf0.fit(train_feats_matrix.toarray(), train_labels.values)\n",
    "    test_pred = clf0.predict(test_feats_matrix.toarray())\n",
    "    \n",
    "    return test_pred\n",
    "\n",
    "\n",
    "############ Naive Bayes related############\n",
    "\n",
    "def normalize(P, smoothing_prior=0):\n",
    "\n",
    "    N = P.shape[0]\n",
    "    \n",
    " \n",
    "    norm = np.sum(P, axis=0, keepdims=True)\n",
    "    \n",
    "\n",
    "    return (P + smoothing_prior) / (norm + smoothing_prior*N)\n",
    "\n",
    "def compute_prior(data_label, data_matrix):\n",
    "\n",
    "    N = data_matrix.shape[0]\n",
    "    K = max(data_label) # labels begin with 1\n",
    "    \n",
    "    ###### Your Code Here ######\n",
    "    \n",
    "    data_label_onehot_matrix = np.zeros((N, K))\n",
    "\n",
    "    for i, l in enumerate(data_label):\n",
    "        # YOUR CODE HERE\n",
    "        data_label_onehot_matrix[i, l-1] = 1\n",
    "\n",
    "    label_freq = np.sum(data_label_onehot_matrix, axis=0, keepdims=False)\n",
    "\n",
    "    # (use 1 as the smoothing prior)\n",
    "    P_y = normalize(label_freq, smoothing_prior=1)\n",
    "\n",
    "    return P_y, data_label_onehot_matrix\n",
    "\n",
    "def compute_likelihood(data_matrix, data_label_onehot_matrix):\n",
    "\n",
    "    ###### Your Code Here ######\n",
    "    word_freq = data_matrix.transpose().dot(data_label_onehot_matrix)\n",
    "\n",
    "    # (use 1 as the smoothing prior)\n",
    "    P_xy = normalize(word_freq,smoothing_prior=1)\n",
    "\n",
    "    return P_xy\n",
    "\n",
    "def  classify_nb(data_matrix, P_y, P_xy):\n",
    "\n",
    "    ###### Your Code Here ######\n",
    "    log_P_y = np.expand_dims(np.log(P_y), axis=0)\n",
    "    log_P_xy = np.log(P_xy)\n",
    "    log_P_dy = data_matrix.dot(log_P_xy)\n",
    "    log_P = log_P_y + log_P_dy\n",
    "    pred = np.argmax(log_P, axis=1) + 1\n",
    "\n",
    "    return pred\n",
    "\n",
    "\n",
    "\n",
    "train_file = \"data/train.csv\"\n",
    "test_file = \"data/test.csv\"\n",
    "ans_file = \"data/answer.csv\"\n",
    "\n",
    "# load data\n",
    "train_ids, train_texts, train_labels = load_data(train_file)\n",
    "test_ids, test_texts, _ = load_data(test_file)\n",
    "test_labels = load_labels(ans_file)\n",
    "\n",
    "# extract features\n",
    "\n",
    "# tokenization\n",
    "train_tokens = [tokenize(text) for text in train_texts]\n",
    "test_tokens = [tokenize(text) for text in test_texts]\n",
    "\n",
    "# stemming\n",
    "train_stemmed = [stem(tokens) for tokens in train_tokens]\n",
    "test_stemmed = [stem(tokens) for tokens in test_tokens]\n",
    "\n",
    "train_stemmed = [filter_stopwords(tokens) for tokens in train_stemmed]\n",
    "test_stemmed = [filter_stopwords(tokens) for tokens in test_stemmed]\n",
    "\n",
    "train_3_gram = [n_gram(tokens, 3) for tokens in train_stemmed]\n",
    "test_3_gram = [n_gram(tokens, 3) for tokens in test_stemmed]\n",
    "\n",
    "# to numerical\n",
    "\n",
    "train_feats_matrix, test_feats_matrix = to_numerical(train_3_gram, test_3_gram, min_thresh=2)\n",
    "\n",
    "# 1. Classify using Logistic Regression\n",
    "\n",
    "\n",
    "test_pred = classify_lr(train_feats_matrix, test_feats_matrix, train_labels)\n",
    "print(\"Test Accuracy of LR:\", accuracy_score(test_labels.values, test_pred))\n",
    "\n",
    "\n",
    "# 2. Naive Bayes\n",
    "\n",
    "P_y, data_label_onehot_matrix = \\\n",
    "compute_prior(train_labels, train_feats_matrix)\n",
    "print('P_y:', P_y)\n",
    "\n",
    "\n",
    "P_xy = compute_likelihood(train_feats_matrix, data_label_onehot_matrix)\n",
    "\n",
    "print('P_xy.shape:', P_xy.shape)\n",
    "\n",
    "\n",
    "train_pred = classify_nb(train_feats_matrix, P_y, P_xy)\n",
    "\n",
    "test_pred = classify_nb(test_feats_matrix, P_y, P_xy)\n",
    "\n",
    "test_acc= accuracy_score(test_labels.values, test_pred)\n",
    "print(\"Test Accuracy of NB:\", test_acc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
